"""
A Retrieval-Augmented Generation (RAG) application for querying financial reports.

This script loads financial documents from a specified directory, processes them into
searchable chunks, and uses a vector store to retrieve relevant information based on
user queries. A large language model (LLM) then generates answers based on the
retrieved context.

Usage:
    python main.py

The script will prompt the user for questions about the financial documents
and display the answers generated by the RAG chain.
"""

# Import necessary libraries
import os
import tempfile
import logging
from dotenv import load_dotenv
from typing import List

# LangChain components for document handling
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import (
    TextLoader,
    DirectoryLoader,
    PyPDFLoader,
)
from langchain.schema import Document

# LangChain components for embeddings and vector stores
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

# LangChain components for the RAG chain
from langchain_openai import ChatOpenAI
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)

# Load environment variables from .env file
load_dotenv()


def load_docs(directory: str) -> List[Document]:
    """Loads all PDF documents from a specified directory.

    Args:
        directory (str): The path to the directory containing the PDF files.

    Returns:
        List[Document]: A list of loaded documents.
    """
    logging.info(f"=== Loading Documents from {directory} ===")
    # Initialize a directory loader for PDF files
    loader = DirectoryLoader(
        path=directory,
        glob="*.pdf",
        loader_cls=PyPDFLoader,
    )
    # Load the documents
    documents = loader.load()
    logging.info(f"Loaded {len(documents)} documents.\n")
    return documents


def splitter(documents: List[Document]) -> List[Document]:
    """Splits a list of documents into smaller chunks.

    Args:
        documents (List[Document]): The list of documents to be split.

    Returns:
        List[Document]: A list of smaller document chunks.
    """
    logging.info("=== Splitting Documents ===")
    # Initialize a text splitter with specific chunk size and overlap
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,
        chunk_overlap=300,
        length_function=len,
        separators=["\n\n", "\n", " ", ""],
    )
    # Split the documents into chunks
    chunks = text_splitter.split_documents(documents)
    logging.info(f"Created {len(chunks)} chunks.\n")
    return chunks


def get_embedding_model() -> HuggingFaceEmbeddings:
    """Initializes and returns a Hugging Face embedding model.

    Returns:
        HuggingFaceEmbeddings: The initialized embedding model.
    """
    logging.info("=== Initializing Embedding Model ===")
    # Specify the pre-trained model to use for embeddings
    MODEL_NAME = "BAAI/bge-base-en-v1.5"
    # Initialize the Hugging Face embedding model
    embeddings = HuggingFaceEmbeddings(
        model_name=MODEL_NAME,
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True},
    )
    logging.info(f"Embedding model: '{MODEL_NAME}' initialized.\n")
    return embeddings


def vector_store(
    chunks: List[Document], embeddings: HuggingFaceEmbeddings
) -> Chroma:
    """Creates or loads a Chroma vector store from document chunks and embeddings.

    Args:
        chunks (List[Document]): The document chunks to be stored.
        embeddings (HuggingFaceEmbeddings): The embedding model to use.

    Returns:
        Chroma: The created or loaded vector store.
    """
    logging.info("=== Storing Chunks in Vector Store ===")
    persist_directory = "./chroma_db"
    try:
        # Attempt to load an existing vector store
        vectorstore = Chroma(
            persist_directory=persist_directory,
            embedding_function=embeddings,
            collection_name="rag_collection",
        )
        if vectorstore._collection.count() > 0:
            logging.info(
                f"Loaded existing vector store from '{persist_directory}' \
                with {vectorstore._collection.count()} vectors.\n"
            )
            return vectorstore
        else:
            # If the collection is empty, raise an exception to create a new one
            raise Exception("Collection exists but it's empty.\n")
    except Exception:
        # If loading fails, create a new vector store
        logging.info(f"No existing vector store found. Creating a new one...")
        vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=embeddings,
            persist_directory=persist_directory,
            collection_name="rag_collection",
        )
        logging.info(
            f"Vector store created and persisted to '{persist_directory}'.\n"
        )
        return vectorstore


def get_llm() -> ChatOpenAI:
    """Initializes and returns the language model.

    Returns:
        ChatOpenAI: The initialized language model.
    """
    logging.info("=== Initializing LLM ===")
    # Initialize the ChatOpenAI model
    llm = ChatOpenAI(model_name="gpt-3.5-turbo")
    logging.info(f"LLM initialized: {llm.model_name}\n")
    return llm


def create_rag_chain(retriever, llm) -> Runnable:
    """Creates a Retrieval-Augmented Generation (RAG) chain.

    Args:
        retriever: The retriever object for fetching relevant documents.
        llm: The language model for generating answers.

    Returns:
        Runnable: The created RAG chain.
    """
    logging.info("=== Creating RAG Chain ===")
    # Define the prompt template for the RAG chain
    prompt_template = """
        You are a highly skilled financial analyst AI.
        Your task is to answer the user's question based *exclusively* on the provided context from the financial report.

        Follow these rules strictly:
        1.  Analyze the context provided below.
        2.  If the answer involves a number, extract it precisely as it appears in the text.
        3.  If the information is not available in the context, you MUST state: 'This information is not found in the provided document.' Do not try to guess or use external knowledge.

        CONTEXT:
        {context}

        QUESTION:
        {question}

        ANSWER:
        """
    prompt = ChatPromptTemplate.from_template(prompt_template)

    def format_docs(docs):
        """Formats a list of documents into a single string."""
        return "\n\n".join(doc.page_content for doc in docs)

    # Create the RAG chain by piping together the components
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    logging.info("RAG Chain created successfully.\n")
    return rag_chain


def main():
    """Main function to set up and run the FinQuery RAG application."""
    # Load documents from the specified directory
    documents = load_docs("data/")

    # Split the documents into smaller chunks
    chunks = splitter(documents)

    # Initialize the embedding model
    embeddings = get_embedding_model()

    # Create or load the vector store
    vectorstore = vector_store(chunks, embeddings)

    # Create a retriever from the vector store
    logging.info("Creating retriever")
    retriever = vectorstore.as_retriever(
        search_type="similarity", search_kwargs={"k": 5}
    )
    logging.info("Retriever instantiated")

    # Instantiate the language model
    llm = get_llm()

    # Create the RAG chain
    rag_chain = create_rag_chain(retriever, llm)

    # Display a welcome message to the user
    print("\n\n" + "=" * 60)
    print("\t\t Welcome to FinQuery! Ask questions about the financial report.\t\t")
    print("\t\t Type 'exit' to quite.\t\t")
    print("=" * 60)

    # Start an interactive loop to get user questions
    while True:
        try:
            user_question = input("\nYour Question: ")
            # Check if the user wants to exit
            if user_question.lower() in ["exit", "quit"]:
                break

            # Skip empty input
            if not user_question.strip():
                continue

            print("\nThinking...\n")
            # Invoke the RAG chain to get the answer
            answer = rag_chain.invoke(user_question)

            # Print the answer
            print("\n=== ANSWER ===\n")
            print(answer)
            print("\n=== END OF ANSWER ===\n")
        except (KeyboardInterrupt, EOFError):
            # Handle user interruption
            print("\nExiting")
            break
    print("\nThank you for using FinQuery!")


if __name__ == "__main__":
    # Run the main function when the script is executed
    main()
