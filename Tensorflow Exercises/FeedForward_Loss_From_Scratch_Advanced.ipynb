{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYI7p5o5MNcDyR63RfB81m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadreza-mohammadi94/AgenticAI/blob/main/Tensorflow%20Exercises/FeedForward_Loss_From_Scratch_Advanced.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "OF2S9ibIRmKG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M6iE2AmdQ_h5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1: Implementing a Deep FFN (3 Layers) with a Class\n",
        "\n",
        "*   **Exercise Goal:**\n",
        "    Build a Feedforward Neural Network with two hidden layers and one output layer using a Python class. This exercise will help you manage parameters and computations in a structured way.\n",
        "*   **Model and Data Specifications:**\n",
        "    *   **Input `X`:** A batch of **128 samples**, each with **50 features**.\n",
        "    *   **True Labels `Y`:** 128 corresponding true numerical values for each sample (regression).\n",
        "    *   **Architecture:**\n",
        "        *   Hidden Layer 1: 32 neurons with **ReLU** activation.\n",
        "        *   Hidden Layer 2: 16 neurons with **ReLU** activation.\n",
        "        *   Output Layer: 1 neuron with **Linear** activation.\n",
        "    *   **Loss Function:** Mean Squared Error (MSE).\n",
        "*   **Your Task:**\n",
        "    1.  Create a class named `DeepFFN`.\n",
        "    2.  In the `__init__` method, take the layer dimensions as input and initialize all model parameters (`W1`, `b1`, `W2`, `b2`, `W3`, `b3`).\n",
        "    3.  Create a `forward` method that takes the input `X`, performs the entire forward propagation process, and returns the `predictions`.\n",
        "    4.  Create a `compute_loss` method that takes `Y_true` and `Y_pred` and calculates the MSE loss.\n",
        "    5.  Create an instance of your class, run the forward pass, and compute the loss.\n",
        "*   **Key Concepts to Learn:**\n",
        "    *   Organizing code using classes (OOP).\n",
        "    *   Separating the logic for initialization, forward pass, and loss calculation.\n",
        "*   **Theoretical Guidance:**\n",
        "    *   The `__init__` method is for defining parameters, and the `forward` method is for defining the computations.\n",
        "    *   Store intermediate values (like `A1`, `A2`) in the `forward` method, as you will need them for backpropagation later.\n"
      ],
      "metadata": {
        "id": "X7B2QQuxRx3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define helper functions\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def mse_loss(y_true, y_pred):\n",
        "    return np.mean(np.square(y_true - y_pred))"
      ],
      "metadata": {
        "id": "c5QT_C2fRyu7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fcee797"
      },
      "source": [
        "### ReLU Activation Function\n",
        "\n",
        "The Rectified Linear Unit (ReLU) activation function is defined as:\n",
        "\n",
        "$$ ReLU(x) = \\max(0, x) $$\n",
        "\n",
        "where $x$ is the input to the neuron."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4fb3c18"
      },
      "source": [
        "### Mean Squared Error (MSE) Loss Function\n",
        "\n",
        "The Mean Squared Error (MSE) loss function is a common loss function used in regression problems. It is calculated as the average of the squared differences between the true values and the predicted values:\n",
        "\n",
        "$$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (Y_{true, i} - Y_{pred, i})^2 $$\n",
        "\n",
        "where:\n",
        "- $n$ is the number of samples.\n",
        "- $Y_{true, i}$ is the true value for the $i$-th sample.\n",
        "- $Y_{pred, i}$ is the predicted value for the $i$-th sample."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DeepFNN class\n",
        "class DeepFFN:\n",
        "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim, seed=42):\n",
        "        \"\"\"\n",
        "        Initilize the parameters (Weights, biases) for a 3 layer FNN.\n",
        "        \"\"\"\n",
        "        # Set random seed\n",
        "        np.random.seed(seed)\n",
        "        self.params = {}\n",
        "        # Layer 1 (DxH) -> W: (50, 32) - b :(1, 32)\n",
        "        self.params[\"W1\"] = np.random.randn(input_dim, hidden_dim1) * 0.01\n",
        "        self.params[\"b1\"] = np.zeros((1, hidden_dim1))\n",
        "        # Layer 2 (DxH) -> W: (32, 16) - b: (1, 16)\n",
        "        self.params[\"W2\"] = np.random.randn(hidden_dim1, hidden_dim2) * 0.01\n",
        "        self.params['b2'] = np.zeros((1, hidden_dim2))\n",
        "        # Output layer\n",
        "        self.params[\"W3\"] = np.random.randn(hidden_dim2, output_dim) * 0.01\n",
        "        self.params[\"b3\"] = np.zeros((1, output_dim))\n",
        "\n",
        "        # check params\n",
        "        print(\"DeepFNN model initialized with the following paramter shapes:\")\n",
        "        for name, param in self.params.items():\n",
        "            print(f\"{name}: {param.shape}\")\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Performs the forward pass calculation.\n",
        "        Stores intermediate values in a cache for backpropagation.\n",
        "        \"\"\"\n",
        "        # unpack params\n",
        "        W1, b1 = self.params[\"W1\"], self.params[\"b1\"]\n",
        "        W2, b2 = self.params[\"W2\"], self.params[\"b2\"]\n",
        "        W3, b3 = self.params[\"W3\"], self.params[\"b3\"]\n",
        "\n",
        "        # Layer 1 calculations\n",
        "        print(\"\\n\\t\\t>>> Forward Pass <<<\\n\")\n",
        "        Z1 = np.dot(X, W1) + b1\n",
        "        A1 = relu(Z1)\n",
        "        print(f\"\\nStep 1.1: Weighted Sum of Hidden Layer 1 (Z1) shape: {Z1.shape}\")\n",
        "        print(f\"Step 1.2: Output of Hidden Layer 1 (A1) shape: {A1.shape}\")\n",
        "\n",
        "        # Layer 2 calculations\n",
        "        Z2 = np.dot(A1, W2) + b2\n",
        "        A2 = relu(Z2)\n",
        "        print(f\"\\nStep 2.1: Weighted Sum of Hidden Layer 2 (Z2) shape: {Z2.shape}\")\n",
        "        print(f\"Step 2.2: Output of Hidden Layer 2 (A2) shape: {A2.shape}\")\n",
        "\n",
        "        # Output layer calculations\n",
        "        Z3 = np.dot(A2, W3) + b3\n",
        "        predictions = Z3\n",
        "        print(f\"\\nStep 3.1: Weighted Sum of Output Layer (Z3) shape: {Z3.shape}\")\n",
        "        print(f\"Step 3.2: Final Predictions shape: {predictions.shape}\")\n",
        "        print(\"\\n\\t\\t >>> Forward Pass Complete <<<\\n\")\n",
        "\n",
        "        # Store intermediate values in cache\n",
        "        self.cache = {\n",
        "            'X': X,\n",
        "            'A1': A1, 'Z1': Z1,\n",
        "            'A2': A2, 'Z2': Z2,\n",
        "            'predictions': predictions\n",
        "        }\n",
        "        return predictions\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Computes the MSE loss between true and predicted values.\n",
        "        \"\"\"\n",
        "        return mse_loss(y_true, y_pred)"
      ],
      "metadata": {
        "id": "KYsIGnXkVmkA"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a07a406"
      },
      "source": [
        "The `DeepFFN` class defines a three-layer Feedforward Neural Network.\n",
        "\n",
        "The `__init__` method is the constructor for this class. It takes the dimensions of the input layer (`input_dim`), two hidden layers (`hidden_dim1` and `hidden_dim2`), and the output layer (`output_dim`) as arguments. It also includes a `seed` argument for reproducibility when initializing random weights.\n",
        "\n",
        "Inside the `__init__` method:\n",
        "- It sets the random seed using `np.random.seed()`.\n",
        "- It initializes an empty dictionary called `self.params` which will store the weights and biases of the network.\n",
        "- It then initializes the weight matrices (`W`) and bias vectors (`b`) for each of the three layers using `np.random.randn()` for weights and `np.zeros()` for biases. The weights are scaled by 0.01 to help with training.\n",
        "- Finally, it prints the shapes of the initialized parameters to the console for verification.\n",
        "\n",
        "This `__init__` method sets up the basic structure and initial parameters of the neural network before any training or forward passes are performed."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "# define configurations\n",
        "BATCH_SIZE = 128\n",
        "FEATURES = 50\n",
        "HIDDEN_LAYER_1_NEURONS = 32\n",
        "HIDDEN_LAYER_2_NEURONS = 16\n",
        "OUTPUT_LAYER_NEURONS = 1\n",
        "\n",
        "# generate synthetic data\n",
        "X_train = np.random.randn(BATCH_SIZE, FEATURES)\n",
        "y_train = np.random.randn(BATCH_SIZE, OUTPUT_LAYER_NEURONS)\n",
        "\n",
        "print(\"\\n\\n\\t\\t >>> Data:<<<\")\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "\n",
        "# Instantiate the model and run\n",
        "# create a instance of the model with define configs\n",
        "my_model = DeepFFN(\n",
        "    input_dim=FEATURES,\n",
        "    hidden_dim1=HIDDEN_LAYER_1_NEURONS,\n",
        "    hidden_dim2=HIDDEN_LAYER_2_NEURONS,\n",
        "    output_dim=OUTPUT_LAYER_NEURONS\n",
        ")\n",
        "\n",
        "# perfome forward pass\n",
        "predictions = my_model.forward(X_train)\n",
        "# compute loss\n",
        "loss = my_model.compute_loss(y_train, predictions)\n",
        "\n",
        "print(f\"Prediciton's Shape: {predictions.shape}\")\n",
        "print(f\"first 5 predictions: {predictions[:5]}\")\n",
        "print(\"\\n\\t\\t >>> Loss <<<\")\n",
        "print(f\"Loss: {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gA_e6o3JXwJC",
        "outputId": "ec37f87d-7e45-4a8e-e8a0-5eb33cb2311e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\t\t >>> Data:<<<\n",
            "X_train shape: (128, 50)\n",
            "y_train shape: (128, 1)\n",
            "DeepFNN model initialized with the following paramter shapes:\n",
            "W1: (50, 32)\n",
            "b1: (1, 32)\n",
            "W2: (32, 16)\n",
            "b2: (1, 16)\n",
            "W3: (16, 1)\n",
            "b3: (1, 1)\n",
            "\n",
            "\t\t>>> Forward Pass <<<\n",
            "\n",
            "\n",
            "Step 1.1: Weighted Sum of Hidden Layer 1 (Z1) shape: (128, 32)\n",
            "Step 1.2: Output of Hidden Layer 1 (A1) shape: (128, 32)\n",
            "\n",
            "Step 2.1: Weighted Sum of Hidden Layer 2 (Z2) shape: (128, 16)\n",
            "Step 2.2: Output of Hidden Layer 2 (A2) shape: (128, 16)\n",
            "\n",
            "Step 3.1: Weighted Sum of Output Layer (Z3) shape: (128, 1)\n",
            "Step 3.2: Final Predictions shape: (128, 1)\n",
            "\n",
            "\t\t >>> Forward Pass Complete <<<\n",
            "\n",
            "Prediciton's Shape: (128, 1)\n",
            "first 5 predictions: [[-4.25245607e-05]\n",
            " [ 4.74184584e-05]\n",
            " [-7.14530409e-05]\n",
            " [-1.15679538e-04]\n",
            " [ 1.69084499e-05]]\n",
            "\n",
            "\t\t >>> Loss <<<\n",
            "Loss: 1.0808246885533759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G3cMNPeRXzSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Test With Tensorflow"
      ],
      "metadata": {
        "id": "ldnJ39gLbUsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This code defines a 3-layer Feedforward Neural Network using TensorFlow's Keras API.\n",
        "# It has two hidden layers with ReLU activation and an output layer with linear activation.\n",
        "keras_model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(units=HIDDEN_LAYER_1_NEURONS, activation='relu', input_shape=(FEATURES,)),\n",
        "    tf.keras.layers.Dense(units=HIDDEN_LAYER_2_NEURONS, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=OUTPUT_LAYER_NEURONS, activation='linear')\n",
        "])\n",
        "\n",
        "# Manually set the weights using the parameters from our manual model\n",
        "# This is the crucial step for a direct comparison.\n",
        "keras_model.layers[0].set_weights([my_model.params['W1'], my_model.params['b1'].flatten()])\n",
        "keras_model.layers[1].set_weights([my_model.params['W2'], my_model.params['b2'].flatten()])\n",
        "keras_model.layers[2].set_weights([my_model.params['W3'], my_model.params['b3'].flatten()])\n",
        "\n",
        "# prediction with keras model\n",
        "predictions_keras = keras_model.predict(X_train)\n",
        "\n",
        "# compute loss\n",
        "loss_keras = tf.keras.losses.MeanSquaredError()(y_train, predictions_keras)\n",
        "\n",
        "print(f\"Prediciton's Shape: {predictions_keras.shape}\")\n",
        "print(f\"first 5 predictions: {predictions_keras[:5]}\")\n",
        "print(\"\\n\\t\\t >>> Loss <<<\")\n",
        "print(f\"Loss: {loss_keras}\")\n",
        "\n",
        "print(f\"Keras Predictions Shape: {predictions_keras.shape}\")\n",
        "print(f\"Keras MSE Loss: {loss_keras.numpy():.6f}\")\n",
        "\n",
        "print(\"\\t\\t >>> Comparison of Results <<<\")\n",
        "# Compare predictions\n",
        "are_predictions_close = np.allclose(predictions_keras, predictions_keras)\n",
        "# Compare losses\n",
        "are_losses_close = np.allclose(loss, loss_keras.numpy())\n",
        "\n",
        "print(f\"Are the predictions from both methods identical? {are_predictions_close}\")\n",
        "print(f\"Are the loss values from both methods identical? {are_losses_close}\")\n",
        "\n",
        "if are_predictions_close and are_losses_close:\n",
        "    print(\"\\nSuccess! The manual class-based implementation perfectly matches the Keras model's results.\")\n",
        "else:\n",
        "    print(\"\\nThere is a discrepancy. Please check the calculations.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RN2Rh9QAbV61",
        "outputId": "b9ff12c2-a650-44d4-d473-d5c3e43a8784"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
            "Prediciton's Shape: (128, 1)\n",
            "first 5 predictions: [[-4.2524531e-05]\n",
            " [ 4.7418474e-05]\n",
            " [-7.1453047e-05]\n",
            " [-1.1567953e-04]\n",
            " [ 1.6908451e-05]]\n",
            "\n",
            "\t\t >>> Loss <<<\n",
            "Loss: 1.080824613571167\n",
            "Keras Predictions Shape: (128, 1)\n",
            "Keras MSE Loss: 1.080825\n",
            "\t\t >>> Comparison of Results <<<\n",
            "Are the predictions from both methods identical? True\n",
            "Are the loss values from both methods identical? True\n",
            "\n",
            "Success! The manual class-based implementation perfectly matches the Keras model's results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QUMGddyvb8Gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UgI8hbD0b8BK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}