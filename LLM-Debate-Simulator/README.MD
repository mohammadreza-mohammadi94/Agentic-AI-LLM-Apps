# ü§ñ LLM Debate Simulator

[](https://www.python.org/downloads/)
[](https://opensource.org/licenses/MIT)

An agentic AI system that simulates a debate between multiple Large Language Models on a controversial topic. The project uses a fourth LLM as an impartial judge to evaluate the arguments based on clarity, logic, and persuasiveness, providing a ranked analysis of each model's reasoning capabilities.

This project showcases a modular, extensible architecture for integrating and orchestrating multiple AI agents for complex comparison tasks.

-----

## ‚ú® Key Features

  - **Multi-LLM Integration**: Easily connect and use models from various providers (e.g., OpenAI, Groq) in a single workflow.
  - **Agentic Roles**: Assign specific roles and stances (pro, con, neutral) to different LLMs using advanced prompt engineering.
  - **AI-Powered Evaluation**: Leverage a "judge" LLM to provide an objective, criteria-based ranking of the debaters' arguments.
  - **Structured JSON Output**: The judge's evaluation is delivered in a clean, parseable JSON format for easy downstream use.
  - **Formatted Reporting**: Automatically generates a detailed Markdown summary of the debate, including rankings and full arguments.
  - **Modular & Extensible**: The codebase is highly modular, making it simple to add new LLMs, change the debate topic, or customize the evaluation criteria.

-----

## üöÄ Sample Output

Here is an example of the `debate_summary.md` file generated by the simulator:

```markdown
# ü§ñ LLM Debate Simulation

## Topic: *Should remote work be the default standard for all tech companies?*

---

### üèÜ Judge's Final Ranking

| Rank | Debater         | Stance  | Model Used         | Judge's Reasoning                                                               |
| :---: | :-------------- | :------ | :----------------- | :------------------------------------------------------------------------------ |
| **1** | Debater_Con     | Con     | `gpt-4o-mini`      | Presented the most logically structured argument with compelling evidence against the motion. |
| **2** | Debater_Neutral | Neutral | `llama3-70b-8192`  | Provided a well-rounded analysis but lacked the persuasive force of a dedicated stance. |
| **3** | Debater_Pro     | Pro     | `llama3-8b-8192`   | Made a passionate and clear case, but the reasoning was less structured than the others. |

---

### üí¨ Full Arguments

#### Argument from Debater_Pro (`pro` stance, using `llama3-8b-8192`)
> Making remote work the default is not just a perk; it's a strategic evolution for the tech industry. It democratizes opportunity, allowing companies to access a global talent pool, not just those within a 30-mile radius of a physical office. This leads to more diverse teams, richer innovation, and increased employee satisfaction...

#### Argument from Debater_Con (`con` stance, using `gpt-4o-mini`)
> While remote work offers flexibility, standardizing it as the default would be a grave mistake. The erosion of company culture is a primary concern. Spontaneous collaboration, mentorship for junior engineers, and the simple act of building trust happen most effectively in person. A fully remote setup risks turning a vibrant team into a collection of isolated freelancers...

#### Argument from Debater_Neutral (`neutral` stance, using `llama3-70b-8192`)
> The debate over mandatory remote work is nuanced. On one hand, it provides unparalleled autonomy and work-life balance, potentially boosting productivity for self-motivated individuals. On the other hand, it presents significant challenges for team cohesion, security, and ensuring equitable access to resources...
```

-----

## üîß Setup and Installation

Follow these steps to set up and run the project on your local machine.

### 1\. Clone the Repository

```bash
git clone https://github.com/your-username/llm-debate-simulator.git
cd llm-debate-simulator
```

### 2\. Install Dependencies

It's recommended to use a virtual environment.

```bash
# Create and activate a virtual environment (optional but recommended)
python -m venv venv
source venv/bin/activate  # On Windows, use `venv\Scripts\activate`

# Install required packages
pip install -r requirements.txt
```

### 3\. Configure Environment Variables

You'll need API keys from the LLM providers you intend to use.

1.  Make a copy of the example environment file:
    ```bash
    cp .env.example .env
    ```
2.  Open the `.env` file and add your secret API keys:
    ```env
    OPENAI_API_KEY="sk-..."
    GROQ_API_KEY="gsk_..."
    ```

-----

## üèÉ‚Äç‚ôÄÔ∏è How to Run

Execute the main script from the root directory to start the debate simulation:

```bash
python main.py
```

The script will:

1.  Query each debater LLM for its argument.
2.  Send the collected arguments to the judge LLM for evaluation.
3.  Print a summary of the results to the console.
4.  Save the detailed results in the `results/` directory:
      - `ranked_debate.json`: The raw JSON output from the judge.
      - `debate_summary.md`: The formatted Markdown report.

-----

## ‚öôÔ∏è Customization

The project is designed to be easily configurable.

  - **Change the Debate Topic**: Modify the `DEBATE_TOPIC` constant in `config.py`.
  - **Use Different Models**: Adjust the `debaters` and `judge` dictionaries in `main.py` to specify different providers or model names.
  - **Refine the Prompts**: Edit the prompt generation functions (`get_debate_prompt`, `get_judge_prompt`) in `prompts.py` to change the behavior of the AI agents.

-----

## ü§ù Contributing

Contributions are welcome\! If you have ideas for new features or improvements, please feel free to fork the repository, create a new branch for your feature, and submit a pull request.

1.  Fork the Project
2.  Create your Feature Branch (`git checkout -b feature/AmazingFeature`)
3.  Commit your Changes (`git commit -m 'Add some AmazingFeature'`)
4.  Push to the Branch (`git push origin feature/AmazingFeature`)
5.  Open a Pull Request

-----

## üìÑ License

This project is licensed under the MIT License. See the `LICENSE` file for details.
